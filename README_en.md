# DongwuLLM: Pre-training, Compressing, Extending, and Distilling Finance LLMs

[[中文版](README.md)] [[English](README_en.md)]

Large Language Models (LLMs), pre-trained on massive textual data, have demonstrated a remarkably broad range of capabilities for various downstream tasks. However, training these models is challenging due to memory constraints. To address this challenge, Soochow Securities has built the DongwuLLM code library and implemented efficient training techniques for large securities models. At the same time, we also explored some other methods, such as compressing large models, expanding the context length of large models, and using synthesized text generated by other pre-trained large models for distillation.

Key features of our work include:

1) **Pre-training and Finetuning**: We offer scripts for creating datasets specifically for LLM training. Additionally, our codebase includes functionality for converting checkpoints between the Llama weights in Huggingface format and the Megatron-LM format. Metrics such as TFlops and token speed are reported using 8xA100-80GB devices.

2) **Context Length Extension**: We utilize Position Interpolation (PI) as detailed in this paper to extend the context length of the Showcai-13b model from 4096 to 32768 tokens and provide corresponding training scripts. Additionally, we provide the Perplexity (PPL) test results on the Pile and PG19 datasets.

3) **Distillation**: We continually train Showcai-13b model using synthesized text generated by other larger LLM models. The results show that using only 10B tokens that a blend of synthetic data and original data (pre-training data at 0.5% of the total) can lead to a 1-point improvement on the comprehensive financial benchmark.

# 内容

- [Setup](#Setup)
- [Training](#Training)
  - [Data processing](#Data-processing)
  - [Checkpoint conversion](#Checkpoint-conversion)
  - [Examples for 31B/65B/132B training](#Examples-for-31B65B132B-training)
- [Context length extension](#Context-length-extension)
  - [Examples for context length extension training](#Examples-for-context-length-extension-training)
- [Distillation](#Distillation)
  - [Generating synthetic data](#Generating-synthetic-data)
  - [Results of distillation](#Results-of-distillation)

## Setup

Similar to Megatron-LM, we strongly recommend using the release of [NGC&#39;s PyTorch container](https://ngc.nvidia.com/catalog/containers/nvidia:pytorch) with DGX nodes. To launch an instance of the PyTorch container, you can follow the steps below:

1. Installing docker and nvidia-docker on your gpu machine.

2. Execute the following Docker commands:

```bash
  docker pull nvcr.io/nvidia/pytorch:23.12-py3
  docker run --gpus all --shm-size=128g --net=host -dit --rm --name megatron -v /your_dir:/your_dir -v /root/.ssh:/root/.ssh nvcr.io/nvidia/pytorch:23.12-py3
```

3. Install sentencepiece and nltk in your environment.

## Training

Following these essential steps to train a large language model:

1. **Data Processing**: Convert textual data into a binary format suitable for training.
2. **Checkpoint Conversion**: If you plan to finetune a pre-trained LLM, such as Llama2-7b, you will need to convert weights from the Huggingface format to the Megatron format. Conversely, to simplify the inference process with your trained LLM, convert the weights from the Megatron format back to the Huggingface format.
3. **Start the pre-training**

### Data processing

The data preprocessing steps align with those outlined in [Megatron-LM](https://github.com/NVIDIA/Megatron-LM?tab=readme-ov-file#data-preprocessing). Your training data should be formatted as jsonl, with each line containing a single json object representing a text sample. Below is an example script for preparing data for Llama training:

```bash
python tools/preprocess_data.py \
    --input /Path/to/my-corpus.jsonl \
    --output-prefix /Path/to/my-corpus \
    --tokenizer-type Llama2Tokenizer \
    --tokenizer-model /Path/to/tokenizer.model \
    --append-eod \
    --workers 16
```

To combine multiple binary-format datasets into a single dataset, execute the following command:

```bash
python tools/merge_datasets.py \
    --input /Path/to/datasets/folder \
    --output-prefix /Path/to/merged/dataset 
```

### Checkpoint conversion

Megatron-LM applies pipeline parallelism and tensor parallelism to enable llm training with limited memory. Sometimes we need to change the number of pipeline parallelism and tensor parallelism in our checkpoint. Here is an example:

```bash
python tools/checkpoint/util.py \
        --model-type GPT \
        --load-dir /Path/to/ckpts/Llama2-7b-tp1 \
        --save-dir /Path/to/ckpts/Llama2-7b-tp4 \
        --target-tensor-parallel-size 4 \
        --target-pipeline-parallel-size 1 \
        --megatron-path /Path/to/Megatron
```

To convert huggingface-format weight into megatron format, here is an example on Llama:

```bash
TP=1
HF_FORMAT_DIR=/Path/to/Llama-2-7b-hf
MEGATRON_FORMAT_DIR=/Path/to/Llama2-7b-tp1
TOKENIZER_MODEL=/Path/to/Llama-2-7b-hf/tokenizer.model

python tools/checkpoint/util.py \
    --model-type GPT \
    --loader llama2_hf \
    --saver megatron \
    --target-tensor-parallel-size ${TP} \
    --load-dir ${HF_FORMAT_DIR} \
    --save-dir ${MEGATRON_FORMAT_DIR} \
    --tokenizer-model ${TOKENIZER_MODEL}
```

To convert huggingface-format weight into megatron format, you should first use the scripts above to convert the megatron checkpoint in pipeline parallelism and tensor parallelism. Here is an example on Llama:

```bash
python tools/checkpoint_conversion/llama_checkpoint_conversion.py \
    --convert_checkpoint_from_megatron_to_transformers \
    --load_path "/Path/to/Llama2-7b-tp1" \
    --save_path "/Path/to/Llama2-7b-hf" \
    --target_params_dtype "bf16" \
    --make_vocab_size_divisible_by 1 \
    --print-checkpoint-structure \
    --megatron-path /Path/to/Megatron
```

### Examples for 31B/65B/132B training

We provide scripts for training [31B](./scripts/llama_31B.sh), [65B](./scripts/llama_65B.sh) and [132B](./scripts/llama_132B.sh) llama-based LLMs. The TFlops and token speed on A100-SXM4-80G is reported in the table below:

|                             | 31B   | 65B   | 132B  |
| --------------------------: | ----- | ----- | ----- |
|         **TFLOP/s per GPU** | 161   | 161   | 177   |
| **Tokens / day 8*A100-80g** | 0.59B | 0.27B | 0.15B |

## Context length extension

LLMs typically have fixed context lengths, such as 2048 for LLaMA models and 4096 for LLaMA2 models. However, these preset context lengths may not suffice for many downstream tasks that require longer context windows, like long conversations or extracting informatrion from long documents. Consequently, extending the context window of pretrained LLMs becomes essential. In this section, we provide tutorials on extending the context window from 4096 to up to 32768 for Showcai-13B using [Position Interpolation](https://arxiv.org/pdf/2306.15595.pdf). Remarkably, with just 1000 steps of continual training, Position Interpolation can achieve high-quality performance in long text modeling.

### Examples for context length extension training

The script for extending the 13B model from 4096 to 32768 tokens can be found [here](./scripts/llama13B_strech_32k.sh). We conducted a series of tests to compare the performance of the original Showcai-13B and the extended context length Showcai-13B in terms of PPL (the smaller the value, the better the modeling ability) on the Pile and PG-19 datasets.

| Model               | Pile-4096 | Pile-32768 | PG19-4096 | PG19-32768 |
| ------------------- | --------- | ---------- | --------- | ---------- |
| original Showcai-GPT | 8.27      | -          | 8.00      | -          |
| Showcai-GPT-32K      | 8.25      | 7.89       | 8.20      | 7.47       |

The results indicate that with position interpolation, just 1000 steps can achieve high quality in long-context language modeling.We further confirmed the model's ability to be maintained through testing on the FinEval dataset: the original model achieved a comprehensive score of 44.91 on FinEval, while the expanded model scored 43.61. This indicates that the model's performance in the financial domain remains largely unchanged after expansion

When converting the Megatron-format checkpoint to the Huggingface-format, remember to adjust the freqs used in rotary embedding. For instance, when using a rotary-seq-len-interpolation-factor of 2, it's necessary to modify the corresponding function in modeling_llama.py:

```python
def _set_cos_sin_cache(self, seq_len, device, dtype):
    self.max_seq_len_cached = seq_len
# t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)
    t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype) / 2.0 
    # Change t with rotary-seq-len-interpolation-factor 

    freqs = torch.einsum("i,j->ij", t, self.inv_freq)
    # Different from paper, but it uses a different permutation in order to obtain the same calculation
    emb = torch.cat((freqs, freqs), dim=-1)
    self.register_buffer("cos_cached", emb.cos()[None, None, :, :].to(dtype), persistent=False)
    self.register_buffer("sin_cached", emb.sin()[None, None, :, :].to(dtype), persistent=False)
```

## Distillation

Distillation is an effective method for transferring the knowledge and capabilities acquired from the pre-training of a large language model to a smaller, custom model. In our approach, We first utilize other large language models to synthesize data, and then use the synthesized data to train our own 13B model. This strategy has resulted in a performance improvement on the [FinEval](https://github.com/SUFE-AIFLM-Lab/FinEval/tree/main) financial test dataset.

### Generating synthetic data

To efficiently generate synthetic data, we utilize [vllm](https://github.com/vllm-project/vllm) for inference. We employ the prefixes of pretraining data as prompts for generating this synthetic data.

### Results of distillation

We evaluated our distilled model on the [FinEval](https://github.com/SUFE-AIFLM-Lab/FinEval/tree/main) dataset, and the results are reported in the table below:

| model                 | Accounting | Certificate | Economy | Finance | Average |
| --------------------- | ---------- | ----------- | ------- | ------- | ------- |
| base-model            | 43.60      | 44.61       | 40.09   | 49.83   | 44.91   |
| distill on 3B tokens  | 40.00      | 49.70       | 42.02   | 47.54   | 45.17   |
| distill on 10B tokens | 43.27      | 47.90       | 38.64   | 51.14   | 45.87   |

### Acknowledgements

The following open-source repositories are used in DongwuLLM：

[Megatron-LM](https://github.com/NVIDIA/Megatron-LM)

[Megatron-LLaMA](https://github.com/alibaba/Megatron-LLaMA)
